# LiteLLM configuration for Raspberry Pi 5
model_list:
  # Kimi 2.5k (primary)
  - model_name: kimi-2.5k
    litellm_params:
      model: moonshot/moonshot-v1-128k
      api_base: https://api.moonshot.cn/v1
      api_key: "${KIMI_API_KEY}"
    model_info:
      mode: "chat"
      max_tokens: 128000
      supports_function_calling: true
      input_cost_per_token: 0.00000006  # $0.06 per million
      output_cost_per_token: 0.00000024  # $0.24 per million
  
  # Kimi 1.8k (cheaper alternative)
  - model_name: kimi-1.8k
    litellm_params:
      model: moonshot/moonshot-v1-8k
      api_base: https://api.moonshot.cn/v1
      api_key: "${KIMI_API_KEY}"
    model_info:
      mode: "chat"
      max_tokens: 8192
      input_cost_per_token: 0.000000012  # $0.012 per million
      output_cost_per_token: 0.000000012  # $0.012 per million
  
  # GPT-4o Mini (OpenAI)
  - model_name: gpt-4o-mini
    litellm_params:
      model: gpt-4o-mini
      api_key: "${OPENAI_API_KEY}"
    model_info:
      mode: "chat"
      max_tokens: 16384
      input_cost_per_token: 0.00000015  # $0.15 per million
      output_cost_per_token: 0.00000060  # $0.60 per million
  
  # Claude 3 Haiku (Anthropic)
  - model_name: claude-3-haiku
    litellm_params:
      model: claude-3-haiku-20240307
      api_key: "${CLAUDE_API_KEY}"
    model_info:
      mode: "chat"
      max_tokens: 4096
      input_cost_per_token: 0.00000025  # $0.25 per million
      output_cost_per_token: 0.00000125  # $1.25 per million
  
  # Local models via Ollama (if running)
  - model_name: local-codellama
    litellm_params:
      model: ollama/codellama:7b
      # api_base: http://host.docker.internal:11434  # COMMENTED OUT FOR NOW
    model_info:
      mode: "chat"
      max_tokens: 4096
      input_cost_per_token: 0.0
      output_cost_per_token: 0.0
  
  - model_name: local-phi
    litellm_params:
      model: ollama/phi:2.7b
      # api_base: http://host.docker.internal:11434  # COMMENTED OUT FOR NOW
    model_info:
      mode: "chat"
      max_tokens: 2048
      input_cost_per_token: 0.0
      output_cost_per_token: 0.0

litellm_settings:
  # Cache settings
  cache: redis
  cache_params:
    type: "redis"
    host: "${REDIS_HOST:-redis}"
    port: 6379
    ttl: 3600  # 1 hour
  
  # Rate limiting for Pi 5
  max_parallel_requests: 2
  rps: 10  # Requests per second
  rpm: 60  # Requests per minute
  
  # Performance optimizations
  drop_params: true
  default_max_tokens: 2000
  num_retries: 2
  request_timeout: 60
  debug_level: "INFO"
  
  # Fallback configuration
  fallbacks:
    - kimi-2.5k
    - kimi-1.8k
    - gpt-4o-mini
    - claude-3-haiku
    - local-codellama

router_settings:
  routing_strategy: "simple-shuffle"
  poll_interval: 5
  cooldown_time: 30
  num_retries: 2
  retry_after: 2
  timeout: 30
  enable_pre_call_checks: true

environment_variables:
  # API Keys
  KIMI_API_KEY: ""
  OPENAI_API_KEY: ""
  CLAUDE_API_KEY: ""
  ANTHROPIC_API_KEY: ""
  
  # Service URLs
  REDIS_HOST: "redis"
  REDIS_PORT: "6379"